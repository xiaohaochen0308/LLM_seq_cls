# Qwen2.5-7B-Instruct æ–‡æœ¬åˆ†ç±»é¡¹ç›®

æœ¬é¡¹ç›®åˆ©ç”¨ `Qwen2.5-7B-Instruct` æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡ `PyTorch` å’Œ `Transformers` åº“ï¼Œç»“åˆ `ModelScope` å¹³å°æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¿«é€Ÿå®ç°é«˜æ•ˆçš„æ–‡æœ¬åˆ†ç±»ã€‚

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![Transformers](https://img.shields.io/badge/Transformers-4.30%2B-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„æ–‡æœ¬åˆ†ç±»å·¥å…·ï¼ŒåŸºäº `Qwen2.5-7B-Instruct` æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šç§æ–‡æœ¬åˆ†ç±»åœºæ™¯ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€æ„å›¾è¯†åˆ«ç­‰ã€‚é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œç”¨æˆ·å¯ä»¥å¿«é€Ÿå®ç°é«˜ç²¾åº¦çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- **æ”¯æŒå¤šç§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡**ï¼šæƒ…æ„Ÿåˆ†æã€ä¸»é¢˜åˆ†ç±»ã€æ„å›¾è¯†åˆ«ç­‰ã€‚
- **åŸºäºå¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹**ï¼šä½¿ç”¨ `Qwen2.5-7B-Instruct` æ¨¡å‹ï¼Œæä¾›é«˜è´¨é‡çš„æ–‡æœ¬åˆ†ç±»èƒ½åŠ›ã€‚
- **ç®€å•æ˜“ç”¨**ï¼šé€šè¿‡å‡ è¡Œä»£ç å³å¯å®Œæˆæ¨¡å‹åŠ è½½å’Œæ¨ç†ã€‚
- **æ”¯æŒè‡ªå®šä¹‰è®­ç»ƒ**ï¼šç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

## ç¯å¢ƒå‡†å¤‡

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install torch
pip install transformers
pip install modelscope
```

## æ¨¡å‹ä¸‹è½½

é€šè¿‡ `ModelScope` ä¸‹è½½ `Qwen2.5-7B-Instruct` æ¨¡å‹ï¼š

```bash
modelscope download --model Qwen/Qwen2.5-7B-Instruct
```

## å¿«é€Ÿå¼€å§‹

### 1. å…‹éš†æœ¬é¡¹ç›®

```bash
git clone https://github.com/your-username/qwen2.5-text-classification.git
cd qwen2.5-text-classification
```

### 2. åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæ–‡æœ¬åˆ†ç±»

åœ¨ `main.py` ä¸­ï¼ŒåŠ è½½æ¨¡å‹å¹¶å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼š

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# è¾“å…¥æ–‡æœ¬
text = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•åˆ†ç±»æ•ˆæœã€‚"

# åˆ†è¯å¹¶è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
inputs = tokenizer(text, return_tensors="pt")

# æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# è·å–é¢„æµ‹ç»“æœ
predicted_class = torch.argmax(logits, dim=1).item()
print(f"é¢„æµ‹ç±»åˆ«: {predicted_class}")
```

### 3. è¿è¡Œä»£ç 

```bash
python main.py
```

## é¡¹ç›®ç»“æ„

```
qwen2.5-text-classification/
â”œâ”€â”€ README.md               # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ main.py                 # ä¸»ç¨‹åºï¼ŒåŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæ–‡æœ¬åˆ†ç±»
â”œâ”€â”€ requirements.txt        # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ data/                   # å­˜æ”¾è®­ç»ƒæˆ–æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
â””â”€â”€ LICENSE                 # é¡¹ç›®è®¸å¯è¯
```

## è‡ªå®šä¹‰è®­ç»ƒ

å¦‚æœéœ€è¦åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ­¥éª¤ï¼š

1. å‡†å¤‡æ•°æ®é›†ï¼Œç¡®ä¿æ•°æ®æ ¼å¼ä¸º `æ–‡æœ¬` å’Œ `æ ‡ç­¾`ã€‚
2. ä½¿ç”¨ `Transformers` çš„ `Trainer` API è¿›è¡Œå¾®è°ƒã€‚
3. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å¹¶åŠ è½½ä½¿ç”¨ã€‚

ç¤ºä¾‹ä»£ç ï¼š

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # æ›¿æ¢ä¸ºä½ çš„è®­ç»ƒæ•°æ®é›†
    eval_dataset=eval_dataset,    # æ›¿æ¢ä¸ºä½ çš„éªŒè¯æ•°æ®é›†
)

trainer.train()
```

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç æˆ–æå‡ºå»ºè®®

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚

## è‡´è°¢

- æ„Ÿè°¢ [ModelScope](https://www.modelscope.cn/) æä¾›çš„ `Qwen2.5-7B-Instruct` æ¨¡å‹ã€‚
- æ„Ÿè°¢ [Hugging Face](https://huggingface.co/) æä¾›çš„ `Transformers` åº“ã€‚

---

å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚å¸Œæœ›æœ¬é¡¹ç›®å¯¹æ‚¨çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æœ‰æ‰€å¸®åŠ©ï¼ ğŸš€
